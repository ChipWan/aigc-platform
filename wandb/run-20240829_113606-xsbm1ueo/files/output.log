  0%|                                                                                                                                                                                                     | 0/214 [00:00<?, ?it/s]/root/autodl-tmp/LLMs/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]




  2%|█▌                                                                 | 5/214 [01:26<51:58, 14.92s/it]





  5%|███                                                               | 10/214 [02:32<45:34, 13.40s/it]




  7%|████▎                                                             | 14/214 [03:25<44:14, 13.27s/it]






  9%|█████████████████▌                                                                                                                                                                          | 20/214 [04:45<42:48, 13.24s/it]




 11%|█████████████████████                                                                                                                                                                       | 24/214 [05:38<42:01, 13.27s/it]





 14%|█████████████████████████▍                                                                                                                                                                  | 29/214 [06:45<41:04, 13.32s/it]





 16%|█████████████████████████████▊                                                                                                                                                              | 34/214 [07:51<39:57, 13.32s/it]





 18%|██████████████████████████████████▎                                                                                                                                                         | 39/214 [08:58<38:44, 13.28s/it]





 21%|██████████████████████████████████████▋                                                                                                                                                     | 44/214 [10:04<37:40, 13.30s/it]





 23%|███████████████████████████████████████████                                                                                                                                                 | 49/214 [11:10<36:31, 13.28s/it]





 25%|███████████████████████████████████████████████▍                                                                                                                                            | 54/214 [12:17<35:19, 13.25s/it]





 28%|███████████████████████████████████████████████████▊                                                                                                                                        | 59/214 [13:23<34:19, 13.29s/it]





 30%|████████████████████████████████████████████████████████▏                                                                                                                                   | 64/214 [14:30<33:09, 13.26s/it]






 33%|█████████████████████████████████████████████████████████████▍                                                                                                                              | 70/214 [15:50<31:57, 13.32s/it]




 35%|█████████████████████████████████████████████████████████████████                                                                                                                           | 74/214 [16:43<31:05, 13.33s/it]





 37%|█████████████████████████████████████████████████████████████████████▍                                                                                                                      | 79/214 [17:49<29:59, 13.33s/it]





 39%|█████████████████████████████████████████████████████████████████████████▊                                                                                                                  | 84/214 [18:56<28:48, 13.30s/it]





 42%|██████████████████████████████████████████████████████████████████████████████▏                                                                                                             | 89/214 [20:02<27:39, 13.27s/it]






 44%|███████████████████████████████████████████████████████████████████████████████████▍                                                                                                        | 95/214 [21:22<26:17, 13.26s/it]




 46%|██████████████████████████████████████████████████████████████████████████████████████▉                                                                                                     | 99/214 [22:15<25:26, 13.27s/it]





 49%|██████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                | 104/214 [23:21<24:16, 13.24s/it]






 51%|████████████████████████████████████████████████████████████████████████████████████████████████                                                                                           | 110/214 [24:41<23:00, 13.27s/it]




 53%|███████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                       | 114/214 [25:34<22:11, 13.32s/it]





 56%|███████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                   | 119/214 [26:41<21:02, 13.29s/it]






 58%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                             | 125/214 [28:01<19:39, 13.26s/it]





 61%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                         | 130/214 [29:07<18:33, 13.25s/it]





 63%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                     | 135/214 [30:14<17:33, 13.34s/it]




 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                 | 139/214 [31:07<16:33, 13.25s/it]





 67%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                             | 144/214 [32:13<15:31, 13.31s/it]






 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                        | 150/214 [33:33<14:12, 13.31s/it]





 72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                   | 155/214 [34:39<13:02, 13.27s/it]





 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                               | 160/214 [35:46<11:59, 13.32s/it]




 77%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                           | 164/214 [36:39<11:06, 13.32s/it]






 79%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                      | 170/214 [38:00<09:48, 13.38s/it]





 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                  | 175/214 [39:06<08:42, 13.40s/it]




 84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                              | 179/214 [40:00<07:48, 13.39s/it]






 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                         | 185/214 [41:20<06:26, 13.32s/it]





 89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                     | 190/214 [42:26<05:19, 13.31s/it]





 91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                | 195/214 [43:33<04:12, 13.31s/it]





 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 200/214 [44:40<03:06, 13.30s/it]





 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏       | 205/214 [45:47<02:00, 13.39s/it]





 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 210/214 [46:53<00:53, 13.39s/it]




100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 214/214 [47:47<00:00, 13.36s/it]Saving model checkpoint to mistral-sft-lora-deepspeed/checkpoint-214
loading configuration file config.json from cache at /root/autodl-tmp/huggingface/hub/models--TinyLlama--TinyLlama_v1.1/snapshots/ff3c701f2424c7625fdefb9dd470f45ef18b02d6/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 32000
}
/root/autodl-tmp/LLMs/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
tokenizer config file saved in mistral-sft-lora-deepspeed/checkpoint-214/tokenizer_config.json
Special tokens file saved in mistral-sft-lora-deepspeed/checkpoint-214/special_tokens_map.json
tokenizer config file saved in mistral-sft-lora-deepspeed/tokenizer_config.json
Special tokens file saved in mistral-sft-lora-deepspeed/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 1368
  Batch size = 8
  0%|                                                                                                                                                                                                     | 0/171 [00:00<?, ?it/s]
[2024-08-29 12:23:59,805] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step214 is about to be saved!
[2024-08-29 12:23:59,838] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: mistral-sft-lora-deepspeed/checkpoint-214/global_step214/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-29 12:23:59,838] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving mistral-sft-lora-deepspeed/checkpoint-214/global_step214/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-29 12:23:59,865] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved mistral-sft-lora-deepspeed/checkpoint-214/global_step214/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-29 12:23:59,866] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving mistral-sft-lora-deepspeed/checkpoint-214/global_step214/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-29 12:23:59,985] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved mistral-sft-lora-deepspeed/checkpoint-214/global_step214/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-29 12:23:59,986] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved mistral-sft-lora-deepspeed/checkpoint-214/global_step214/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt



















































































 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 169/171 [02:48<00:01,  1.03it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 214/214 [50:42<00:00, 13.36s/it]Saving model checkpoint to mistral-sft-lora-deepspeed/checkpoint-214
[2024-08-29 12:26:55,151] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step214 is about to be saved!
[2024-08-29 12:26:55,182] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: mistral-sft-lora-deepspeed/checkpoint-214/global_step214/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-29 12:26:55,183] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving mistral-sft-lora-deepspeed/checkpoint-214/global_step214/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-29 12:26:55,209] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved mistral-sft-lora-deepspeed/checkpoint-214/global_step214/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-29 12:26:55,210] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving mistral-sft-lora-deepspeed/checkpoint-214/global_step214/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-29 12:26:55,305] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved mistral-sft-lora-deepspeed/checkpoint-214/global_step214/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-29 12:26:55,305] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved mistral-sft-lora-deepspeed/checkpoint-214/global_step214/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-29 12:26:55,311] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step214 is ready now!
{'train_runtime': 3050.7231, 'train_samples_per_second': 2.245, 'train_steps_per_second': 0.07, 'train_loss': 1.3285615043105365, 'epoch': 1.0}
loading configuration file config.json from cache at /root/autodl-tmp/huggingface/hub/models--TinyLlama--TinyLlama_v1.1/snapshots/ff3c701f2424c7625fdefb9dd470f45ef18b02d6/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 32000
}
/root/autodl-tmp/LLMs/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
tokenizer config file saved in mistral-sft-lora-deepspeed/checkpoint-214/tokenizer_config.json
Special tokens file saved in mistral-sft-lora-deepspeed/checkpoint-214/special_tokens_map.json
tokenizer config file saved in mistral-sft-lora-deepspeed/tokenizer_config.json
Special tokens file saved in mistral-sft-lora-deepspeed/special_tokens_map.json
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 214/214 [50:46<00:00, 14.24s/it]
Waiting for the current checkpoint push to be finished, this might take a couple of minutes.
No files have been modified since last commit. Skipping to prevent empty commit.
Saving model checkpoint to mistral-sft-lora-deepspeed
loading configuration file config.json from cache at /root/autodl-tmp/huggingface/hub/models--TinyLlama--TinyLlama_v1.1/snapshots/ff3c701f2424c7625fdefb9dd470f45ef18b02d6/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 32000
}
/root/autodl-tmp/LLMs/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
tokenizer config file saved in mistral-sft-lora-deepspeed/tokenizer_config.json
Special tokens file saved in mistral-sft-lora-deepspeed/special_tokens_map.json
Saving model checkpoint to mistral-sft-lora-deepspeed
loading configuration file config.json from cache at /root/autodl-tmp/huggingface/hub/models--TinyLlama--TinyLlama_v1.1/snapshots/ff3c701f2424c7625fdefb9dd470f45ef18b02d6/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 32000
}
tokenizer config file saved in mistral-sft-lora-deepspeed/tokenizer_config.json
Special tokens file saved in mistral-sft-lora-deepspeed/special_tokens_map.json
Dropping the following result as it does not have all the necessary fields:
{'dataset': {'name': 'generator', 'type': 'generator', 'config': 'default', 'split': 'train', 'args': 'default'}}